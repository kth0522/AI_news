{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe80NEyP2TG7bCLyGBu1KS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kth0522/AI_news/blob/main/voice_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**드라이브 마운트**"
      ],
      "metadata": {
        "id": "0fswKFro_Cu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMlozMkZf6eC",
        "outputId": "26f44aaa-45af-4781-b9c5-8b2fe49bb592"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/drive/MyDrive/인공지능융합원/\""
      ],
      "metadata": {
        "id": "GbGg08sggci7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**라이브러리 설치**"
      ],
      "metadata": {
        "id": "7w8mLJDjLV48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI4CfmfGLBBj",
        "outputId": "653edfec-2212-45a5-c3a0-739a3309b59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.5)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (0.0.post11)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pathlib\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**코드**"
      ],
      "metadata": {
        "id": "x5hY9Uyg_Hnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from transformers import AutoConfig, Wav2Vec2Processor\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "from transformers.file_utils import ModelOutput\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
        "    Wav2Vec2PreTrainedModel,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    is_apex_available,\n",
        ")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Union\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "from typing import Any, Dict, Union\n",
        "from transformers import AutoConfig, Wav2Vec2Processor\n",
        "import torch\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "\n",
        "import librosa\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "KKDI7ST7MLsI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SpeechClassifierOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "metadata": {
        "id": "qQYkz0CRWbZu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2Vec2ClassificationHead(nn.Module):\n",
        "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.final_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.pooling_mode = config.pooling_mode\n",
        "        self.config = config\n",
        "\n",
        "        self.wav2vec2 = Wav2Vec2Model(config)\n",
        "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "\n",
        "    def merged_strategy(\n",
        "            self,\n",
        "            hidden_states,\n",
        "            mode=\"mean\"\n",
        "    ):\n",
        "        if mode == \"mean\":\n",
        "            outputs = torch.mean(hidden_states, dim=1)\n",
        "        elif mode == \"sum\":\n",
        "            outputs = torch.sum(hidden_states, dim=1)\n",
        "        elif mode == \"max\":\n",
        "            outputs = torch.max(hidden_states, dim=1)[0]\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_values,\n",
        "            attention_mask=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "            labels=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        outputs = self.wav2vec2(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = outputs[0]\n",
        "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
        "        logits = self.classifier(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SpeechClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "s_3-V-QsWojI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_wav_files(folder_path):\n",
        "    wav_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
        "    return wav_files\n",
        "\n",
        "def process_and_predict(wav_file, processor, model, device):\n",
        "    speech_array, sampling_rate = torchaudio.load(wav_file)\n",
        "    if speech_array.shape[0] > 1:  # 스테레오 채널이 있는 경우\n",
        "        speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "    speech_array = speech_array.squeeze().numpy()\n",
        "    resampled_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate,\n",
        "                                       target_sr=processor.feature_extractor.sampling_rate)\n",
        "    features = processor(resampled_array, sampling_rate=processor.feature_extractor.sampling_rate,\n",
        "                         return_tensors=\"pt\", padding=True)\n",
        "    input_values = features.input_values.to(device)\n",
        "    attention_mask = features.attention_mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values, attention_mask=attention_mask).logits\n",
        "    pred_id = torch.argmax(logits, dim=-1).item()\n",
        "    return pred_id\n",
        "\n",
        "def file_number(file_name):\n",
        "    base_name = os.path.basename(file_name)\n",
        "    number_part = os.path.splitext(base_name)[0]\n",
        "    try:\n",
        "        return int(number_part)\n",
        "    except ValueError:\n",
        "        return float('inf')\n",
        "\n",
        "def classify_emotions_in_subfolders(input_dir, output_dir, model, processor, device, config):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for folder_name in os.listdir(input_dir):\n",
        "        folder_path = os.path.join(input_dir, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            wav_files = load_wav_files(folder_path)\n",
        "            wav_files.sort(key=file_number)\n",
        "            predictions = []\n",
        "            for wav_file in wav_files:\n",
        "                pred_id = process_and_predict(wav_file, processor, model, device)\n",
        "                pred_label = config.id2label[pred_id]\n",
        "                file_name_without_extension = os.path.splitext(os.path.basename(wav_file))[0]\n",
        "                predictions.append((file_name_without_extension, pred_label))\n",
        "\n",
        "            df = pd.DataFrame(predictions, columns=['wav_file', 'predicted_emotion'])\n",
        "            output_file = os.path.join(output_dir, f\"{folder_name}_emotions.csv\")\n",
        "            df.to_csv(output_file, index=False)\n",
        "            print(f\"Results for folder '{folder_name}' saved to {output_file}\")"
      ],
      "metadata": {
        "id": "H9CBLN0G3U9N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**실행**"
      ],
      "metadata": {
        "id": "4N4Gx0r0_Y_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**emotion labels** \\\n",
        "AI Hub 데이터셋\\\n",
        "['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']"
      ],
      "metadata": {
        "id": "GWF4A10jhmmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']\n",
        "num_labels = len(label_list)\n",
        "print(label_list)\n",
        "print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw9jFv_yjT84",
        "outputId": "7fea04dd-0d35-458e-cf35-014d615ba345"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path에 구글 드라이브에 올려놓은 voice_emotion_model_weight 경로 넣으면 됨\n",
        "model_path = ROOT_DIR +'voice_emotion_model_weight/'\n",
        "pooling_mode = 'mean'\n",
        "config_name = \"lighteternal/wav2vec2-large-xlsr-53-greek\""
      ],
      "metadata": {
        "id": "vDppvwBrhrgA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_path,\n",
        "    num_labels=num_labels,\n",
        "    label2id={label: i for i, label in enumerate(label_list)},\n",
        "    id2label={i: label for i, label in enumerate(label_list)},\n",
        "    finetuning_task=\"wav2vec2_clf\",\n",
        ")\n",
        "setattr(config, 'pooling_mode', pooling_mode)"
      ],
      "metadata": {
        "id": "ak2JmtVLhlnY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(config_name)\n",
        "sampling_rate = processor.feature_extractor.sampling_rate\n",
        "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_path, config=config,\n",
        "                                                            ignore_mismatched_sizes=False).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFZrWgh4kM3s",
        "outputId": "3804e140-a407-4a3a-ca27-ff15506a46e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/인공지능융합원/voice_emotion_model_weight/ were not used when initializing Wav2Vec2ForSpeechClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at /content/drive/MyDrive/인공지능융합원/voice_emotion_model_weight/ and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR = ROOT_DIR + 'voice_segments'\n",
        "OUTPUT_DIR = ROOT_DIR + 'voice_emotions'"
      ],
      "metadata": {
        "id": "D0u22Qycor0u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_emotions_in_subfolders(INPUT_DIR, OUTPUT_DIR, model, processor, device, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UGxTkfj3ZGm",
        "outputId": "7c5aa710-a2be-4631-ec74-9a6ea79c0db5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for folder 'mbn_ai_7' saved to /content/drive/MyDrive/인공지능융합원/voice_emotions/mbn_ai_7_emotions.csv\n",
            "Results for folder 'mbn_ai_6' saved to /content/drive/MyDrive/인공지능융합원/voice_emotions/mbn_ai_6_emotions.csv\n"
          ]
        }
      ]
    }
  ]
}